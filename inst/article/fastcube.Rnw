\documentclass[12pt,english]{article}
\input{./header/header}

\begin{document}

\maketitle

<<setup, include=FALSE>>=
devtools::load_all(".")
library(ggplot2)
library(sampling)
library(gridExtra)
library(ggvoronoi)
library(grid)
library(lattice)
library(ggplot2)
library(ggrepel)
library(scico)
library(knitr)

library(kableExtra)
library(magrittr)
library(plyr)

# theme plot of the paper
theme_wave <- function(...) {
  theme_minimal() +
    theme(
      text = element_text(family="sans",color = "black",size = 9),
      panel.spacing = unit(2, "lines"),
      # title
      plot.title = element_text(hjust = 0.5,size = 9),
      # axes
      axis.line=element_blank(),
      axis.ticks=element_blank(),
      # legend
      legend.position="bottom",
      legend.title = element_text(size = 9,vjust = +1.0),
      legend.key.size = unit(0.3, "cm"),
      legend.key.width = unit(0.7,"cm") ,
      # background colors
      panel.background=element_blank(),
      panel.border=element_rect(colour = "black",fill = "transparent"),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      # keep edge black facet_wrap
      # strip.background = element_rect(fill="white"),
      strip.text =element_text(color = "black",size = 8)
      )
}


knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)})
opts_chunk$set(fig.path='figure/image-', cache.path='cache/latex-')
options(tikzDocumentDeclaration = "\\documentclass{article}")

pathresults <- file.path(getwd(),"results/",fsep ="/")
if(!dir.exists(pathresults)){
   dir.create(pathresults)
}


thm = knit_theme$get("fine_blue")  # parse the theme to a list
knit_theme$set(thm)

@


\begin{abstract}


\textbf{Key words}: optimal design, spread sampling, stratification
\end{abstract}
\newpage

%-----------------------------------------------------------------------------------
% Introduction
%-----------------------------------------------------------------------------------

\section{Introduction}

%-----------------------------------------------------------------------------------
% Notation
%-----------------------------------------------------------------------------------

\section{Notation}


Consider a finite population $U$ of size $N$ whose units can be defined by labels $k\in\{1,2,\dots,N\}$. Let $\mathcal{S} = \{s | s\subset U\}$ be the set of all possible samples. A sampling design is defined by a probability distribution $p(.)$ on $\mathcal{S}$ such that

$$
p(s) \geq 0 \text{ for all } s\in \mathcal{S} \text{ and }\sum_{ s\in \mathcal{S}}p(s) = 1.
$$

A random sample $S$ is a random vector that maps elements of $\mathcal{S}$ to an $N$ vector of 0 or 1 such that $\textrm{P}(S = s) =
p(s)$. Define $a_k(S)$, for $k = 1,\dots,N$:

$$
\aaa_k =
\left\{\begin{array}{lll} 1 & \text{ if } k\in S\\ 0 & \text{ otherwise} . \end{array} \right.
$$

Then a sample can be denoted by means of a vector notation:
$
 \aaag^\top = (\aaa_1,\aaa_2,\dots,\aaa_N).
$ For each unit of the population, the inclusion probability $0\leq\pi_k\leq 1$ is defined as the probability that unit $k$ is selected into sample $S$:
\begin{equation*}\label{eq:pik}
 \pi_k = \textrm{P}(k \in S) = \textrm{E}(\aaa_k) =  \sum_{s\in S | k \in s} p(s), \text{ for all } k\in U.
\end{equation*}

Let $\pig^\top=(\pi_1,\dots,\pi_N)$ be the vector of inclusion probabilities. Then, $\textrm{E}({\aaag})=\pig.$ Let also $\pi_{k\ell}$ be the probability of selecting the units $k$ and $\ell$ together in the sample, with $\pi_{kk} = \pi_k$. The matrix of second-order inclusion probabilities is given by $\Pi = \E(\aaag\aaag^\top)$. The sample is generally selected with the aim of estimating some population parameters. Let $y_k$ denote a real number associated with unit $k\in U$, usually called the variable of interest. For example, the total
$$
Y=\sum_{k\in U} y_k
$$
can be estimated by using the classical Horvitz-Thompson estimator of the total defined by
\begin{equation}\label{eq:HT}
\widehat{Y}_{HT} = \sum_{k\in U} \frac{y_k a_k}{\pi_k}.
\end{equation}



%-----------------------------------------------------------------------------------
%	Balanced Sampling
%-----------------------------------------------------------------------------------

\section{Balanced Sampling}

Usually, some auxiliary information $\xb_k^\top = (x_{k1},x_{k2},\dots,x_{kq}) \in\mathbb{R}^q$  regarding the population units is available. A sampling design is said to be balanced on the auxiliary variables $x_k$ if and only if it satisfies the balancing equations
\begin{equation*}\label{eq:balance}
  \widehat{\Xb} = \sum_{k\in S} \frac{\xb_k}{\pi_k}  =  \sum_{k\in U} \frac{\xb_k a_k}{\pi_k}= \sum_{k\in U} \xb_k = \Xb.
\end{equation*}
Sometimes it is not possible to select a sample that satisfies exactly the constraint. We write $\widehat{\Xb} \approx \Xb$  to notice that the sample is approximately balanced. In many applications, inclusion probabilities are such that samples have a fixed size $n$. A sampling design of fixed size can be viewed as balanced on only one auxiliary variable $x_k = \pi_k$. Indeed, we have mathematically,
\begin{equation*}
\sum_{k\in S} \frac{x_k}{\pi_k} = \sum_{k \in S}\frac{\pi_k}{\pi_k} =  n_S.
\end{equation*}
Let denote the set of all samples that have fixed size equal to $n$ by
 \begin{equation*}\label{eq:sn} \mathcal{S}_n = \left\{ \aaag\in \{0,1\}^N ~~\bigg|~~ \sum_{k
= 1}^N \aaa_k = n \right\} .
 \end{equation*}
More generally, we write the problem of selecting a balanced sample by the following linear system :
\begin{equation*}\left\{\begin{array}{lll}
\displaystyle\sum_{k\in U}\frac{\xb_k a_k}{\pi_k} =\sum_{k\in U} \frac{\xb_k}{\pi_k}\pi_k\\
a_k \in\{0,1\}, ~ k\in U.

\end{array}\right.
\end{equation*}
Or also written in matrix form,
\begin{equation}\label{eq:bal} \Ab\ab = \Ab\pig,
\end{equation}
where $\Ab= \left( \frac{\xb_1}{\pi_1},\dots, \frac{\xb_N}{\pi_N}\right)$. The aim consist then of obtaining a sample $\ab$ that satisfies the constraints.

%-----------------------------------------------------------------------------------
%	Cube Method
%-----------------------------------------------------------------------------------
\section{Cube Method}

\cite{dev:til:04a} developed the cube method. It selects a sample that is balanced and respect the inclusion probabilities. The method can take equal or unequal inclusion probabilities. A each step , vector $\pig$ is randomly modified. The subspace induced by the linear system \eqref{eq:bal} is defined by the following,

$$\begin{array}{cll}\mathcal{A} &=& \left\{ \ab \in \R^N | \Ab\ab = \Ab\pig \right\}\\
&=& \pig + \text{Null}(\Ab),
\end{array}
$$
where $\text{Null}(\Ab) = \left\{u \in \R^N | \Ab\ub = 0 \right\}$. The idea is then to use a vector of the null space of $\Ab$ such that we ensure to have martingale property of the updated inclusion probabilities. More specifically we have the following equation,
$$ \E_p(\pig^t | \pig^{t-1}) = \E_p(\pig^{t-1}), \text{ for all t = 1,\dots, N}.$$
At each step, at least one component is set to 0 or 1. Matrix A is updated from the new inclusion probabilities. This step is repeated until there is only one component that is not equal to 0 or 1. Algorithm \ref{algo:cube} present the full picture of the method. \cite{cha:til:06} have improved the time consuming cost by using a sub-matrix of smaller size to find a vector that is inside of the null space of $\Ab$. In the next section we present the proposed strategy to improved even more this cost.

\begin{algorithm}
\caption{fast flight phase of the cube Method}\label{algo:cube}
Calculate at first $i$ the number of inclusion probabilities that are not equal to 0 or 1. Let $\pig$ be equal to the $i$ corresponding inclusion probabilities and initializing $\pig^1$ by $\pig$. For $t = 1,\dots,N$, we repeat :

\begin{enumerate}

\item Find $\widetilde{\pig}^t$ the first $J$ entries of the inclusion probabilities $\pig^t$, where $J = \min(p+1,i)$. Define $\Bb$ as the $J$ corresponding rows of the matrix $A$. Notice that the matrix $\Bb$ is either a $(p+1)\times p$ matrix or a $i \times p$ matrix.


\item Find a non null vector $\widetilde{\ub}^t$ inside of the null space of $\Bb$. Define $\ub^t$ as the expanded null vector such that $u_k^t = 0$ for all entry that is not equal to the corresponding $J$ values.

\item Calculate $\widetilde{\lambda_1^t}$ and $\widetilde{\lambda_2^t}$ the two greater value such that
$$\begin{array}{ccccc} 0 &\leqslant & \pi_k^{t} + \lambda_1^t u_k^t \leqslant 1,\\
0 &\leqslant & \pi_k^{t} - \lambda_2^t u_k^t \leqslant 1,\\
 \end{array} \text{ for all } k \in U$$
Observe that $\lambda_1^t $ and $\lambda_2^t$ are both greater than 0.
 \item Update the inclusion probabilities using the rules :
 $$\pig^{t+1} = \left\{\begin{array}{cccc}
 \pig^{t} + \widetilde{\lambda_1^t}\ub^t & \text{ with probability } & q_1^t\\
 \pig^{t} - \widetilde{\lambda_2^t}\ub^t & \text{ with probability } & q_2^t
 \end{array}\right.$$
 where $q_1^t = \widetilde{\lambda_2^t}/(\widetilde{\lambda_1^t} + \widetilde{\lambda_2^t})$ and  $q_2^t = \widetilde{\lambda_1^t}/(\widetilde{\lambda_1^t} + \widetilde{\lambda_2^t})$.
\item Update $i$ the number of inclusion probabilities not equal to 0 or 1.
\end{enumerate}
We repeat these steps until it is no more possible to find a vector $ \widetilde{\ub}^t$ that is inside of the null space.
\end{algorithm}




%-----------------------------------------------------------------------------------
%	Reduction
%-----------------------------------------------------------------------------------
\section{Reduction}

%-----------------------------------------------------------------------------------
%	Simulation
%-----------------------------------------------------------------------------------

\section{Simulation}

%-----------------------------------------------------------------------------------
%	Discussion
%-----------------------------------------------------------------------------------

\section{Discussion}

\bibliography{bibyves}
\bibliographystyle{apalike}

\end{document}
